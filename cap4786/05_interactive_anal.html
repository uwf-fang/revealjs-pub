
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <title>Introduction to Big Data Analytics</title>
    <link rel="stylesheet" type="text/css" href="../_static/revealjs4/dist/reveal.css" />
    <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/revealjs4/plugin/highlight/zenburn.css" />
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

  </head><body>
    <div class="reveal">
        <div class="slides" role="main">
            <section data-background-color="white" data-background-image="../_static/logo.png" data-background-size="50%" data-background-position="top center" data-background-repeat="no-repeat">
<h1>Introduction to Big Data Analytics</h1>
<div class="subtitle docutils container">
<p><strong>Chapter 4: Interactive Data Analysis with Spark</strong></p>
<p>Xingang (Ian) Fang</p>
</div>
</section>
<section >
<h2>Overview</h2>
<ul class="simple">
<li><p>Note: the text book only covers the basics of Spark shell and I will cover
more examples and details about data analysis</p></li>
<li><p>Motivation</p></li>
<li><p>Notebook interface over Spark shell</p></li>
<li><p>Introduction to interactive data processing and analysis</p></li>
</ul>
</section>
<section >
<h2>Motivation</h2>
<ul class="simple">
<li><p>Definition: Interactive data processing and analysis is the process of
interactively querying, analyzing, and visualizing data.</p></li>
<li><p>Why interactive?</p>
<ul>
<li><p>Checking feedback before proceeding to the next step</p></li>
<li><p>Interactive debugging and testing according to the feedback</p></li>
<li><p>One-shot data data processing and analysis</p></li>
<li><p>The counterpart of batch processing which runs sophisticated algorithms
end-to-end repeatedly</p></li>
</ul>
</li>
</ul>
</section>
<section >
<h2>Interfaces</h2>
<ul class="simple">
<li><p>Spark shell: traditional command-line interface</p>
<ul>
<li><p>REPL (Read-Eval-Print Loop) interface</p></li>
<li><p>You need to save and manage your code and outcome separately (e.g. copy and
paste an editor)</p></li>
<li><p>Awkward to reuse code snippets</p></li>
</ul>
</li>
<li><p>Notebook interface: a web-based interface</p>
<ul>
<li><p>Mostly based on Jupyter Notebook</p></li>
<li><p>Require backend (local or remote) to run the code snippets</p></li>
<li><p>Cells to keep both documentation and code snippets</p></li>
<li><p>Run code snippets in cells and see the outcome immediately</p></li>
<li><p>Easy import/export of code snippets to code files or html files</p></li>
<li><p>Convenient to give and collect assignments in teaching</p></li>
</ul>
</li>
</ul>
</section>
<section>
<section >
<h2>Interactive Data Processing and Analysis</h2>
<ul class="simple">
<li><p>Steps to data processing and analysis</p>
<ul>
<li><p>Load data</p></li>
<li><p>Process data</p></li>
<li><p>Analyze data</p></li>
<li><p>Visualize data</p></li>
<li><p>Save report and visualizations</p></li>
<li><p>Save data</p></li>
</ul>
</li>
<li><p>When you need to check the outcome of each step, it is an interactive data
processing and analysis</p></li>
<li><p>Common workflow in data science</p>
<ul>
<li><p>Collect new data</p></li>
<li><p>Perform interactive data processing and analysis as an exploratory data
analysis (EDA)</p></li>
<li><p>Develop a batch processing pipeline based on the EDA</p></li>
<li><p>Deploy the pipeline to production</p></li>
</ul>
</li>
</ul>
</section>
<section >
<h3>Load Data</h3>
<ul class="simple">
<li><p>Artificial data, e.g. <code class="code docutils literal notranslate"><span class="pre">val</span> <span class="pre">data</span> <span class="pre">=</span> <span class="pre">(1</span> <span class="pre">to</span> <span class="pre">1000).toList</span></code></p></li>
<li><p>Load data from a file</p>
<ul>
<li><p>File sources</p>
<ul>
<li><p>local file system</p></li>
<li><p>distributed file system (e.g. HDFS, DBFS)</p></li>
<li><p>file hosted online (access via URL)</p>
<ul>
<li><p>Direct read from URL (if you only read once)</p></li>
<li><p>Download to file system and read</p></li>
</ul>
</li>
</ul>
</li>
<li><p><code class="code docutils literal notranslate"><span class="pre">SparkContext</span></code> provides methods to load files</p>
<ul>
<li><p><code class="code docutils literal notranslate"><span class="pre">textFile</span></code> to read simple text files and return an RDD</p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">read</span></code> to read structured text files (e.g. csv, json, parquet) and
return a DataFrame</p></li>
</ul>
</li>
<li><p>Avoid the Scala native file I/O methods! They does not support distributed
file system!</p></li>
</ul>
</li>
</ul>
</section>
<section >
<h3>Loading/saving Files in Databricks Community Edition</h3>
<ul class="simple">
<li><p>Your code runs in a disposable container (compute). Thus, data stored in the
local file system will be lost after the container is recycled</p></li>
<li><p>Databricks provides a distributed file system called Databricks File System
(DBFS)</p>
<ul>
<li><p>Not accessible in Web Terminal!!! Use the notebook interface instead</p></li>
<li><p>All computes have access to the same DBFS</p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">dbfs:</span></code> prefix to access the files in DBFS formally but this prefix
can be omitted. In spark, you can access the files in DBFS as if they are
in the local file system, although you cannot find the files in the local
file system using the terminal. <strong>Note: single slash used after dbfs:</strong></p>
<ul>
<li><p>E.g. <code class="code docutils literal notranslate"><span class="pre">val</span> <span class="pre">data</span> <span class="pre">=</span> <span class="pre">sc.textFile(&quot;dbfs:/FileStore/tables/data.txt&quot;)</span></code></p></li>
<li><p>E.g. <code class="code docutils literal notranslate"><span class="pre">val</span> <span class="pre">data</span> <span class="pre">=</span> <span class="pre">sc.textFile(&quot;/FileStore/tables/data.txt&quot;)</span></code></p></li>
</ul>
</li>
<li><p>Files stored in <code class="code docutils literal notranslate"><span class="pre">dbfs:/FileStore</span></code> are accessible in the notebook
interface.</p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">SparkContext</span></code> provides methods to read files in DBFS</p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">dbutils.fs</span></code> library to handle the files in DBFS</p></li>
</ul>
</li>
<li><p>There are other protocols such as <code class="code docutils literal notranslate"><span class="pre">hdfs:</span></code> for Hadoop distributed file
system, <code class="code docutils literal notranslate"><span class="pre">s3:</span></code> for Amazon S3, <code class="code docutils literal notranslate"><span class="pre">wasb:</span></code> for Azure Blob Storage, etc.</p>
<ul>
<li><p><strong>Note: these protocols should be followed by double slashes, e.g.
:code:`s3://my-bucket/my-file`</strong></p></li>
</ul>
</li>
</ul>
</section>
<section >
<h3>Loading/Saving Files in Spark Shell in Docker</h3>
<ul>
<li><p>The Spark shell in Docker container has access to the local file system</p></li>
<li><p>Store the data file in your local file system and bind the directory to the
container</p>
<ul>
<li><p>Make a working directory in your computer</p></li>
<li><p>Store the files there</p></li>
<li><p>Run the Spark docker container with the <code class="code docutils literal notranslate"><span class="pre">-v</span></code> option</p>
<pre><code data-trim data-noescape class="bash">docker run -it -v $(pwd):/opt/spark/work-dir --name spark spark:latest /opt/spark/bin/spark-shell</code></pre>
</li>
<li><p>Read and write the files in the Spark shell under the director:
<code class="code docutils literal notranslate"><span class="pre">/opt/spark/work-dir</span></code></p></li>
</ul>
</li>
</ul>
</section>
<section >
<h3>Process Data</h3>
<ul class="simple">
<li><p>Data processing is the process of transforming data into a more useful format
in the future analysis</p></li>
<li><p>Spark data APIs</p>
<ul>
<li><p>RDD (Resilient Distributed Dataset): low-level API</p></li>
<li><p>DataFrame: high-level API for structured data</p></li>
<li><p>DataSet: high-level API for structured data with type safety</p></li>
</ul>
</li>
<li><p>Unstructured data (e.g. text files) can be loaded as RDD</p></li>
<li><p>Structured data (e.g. csv, json, parquet) can be loaded as DataFrame</p></li>
<li><p>Data processing</p>
<ul>
<li><p>Data merging</p></li>
<li><p>Data cleaning (e.g. missing value imputation, outlier detection)</p></li>
<li><p>Unstructured to structured (e.g. parsing, tokenizing, etc.)</p></li>
<li><p>Data transformation (structured to structured, e.g. pivot table, feature
extraction, normalization, etc.)</p></li>
</ul>
</li>
<li><p>FYI: many deep learning models can take unstructured data without much
processing</p></li>
</ul>
</section>
<section >
<h3>Analyze Data</h3>
<ul class="simple">
<li><p>Big data applications usually start with big data size</p></li>
<li><p>After data processing, the data can be either still big or small in size</p></li>
<li><p>For small data, use the traditional data analysis tools (e.g. pandas, numpy)</p></li>
<li><p>For big data</p>
<ul>
<li><p>Distributed tools</p>
<ul>
<li><p>Spark MLlib: machine learning library</p></li>
<li><p>Spark SQL: SQL-like query language can be used to perform simple
statistical analysis</p></li>
</ul>
</li>
<li><p>Deep learning: TensorFlow, PyTorch, etc.</p></li>
</ul>
</li>
</ul>
</section>
</section>

        </div>
    </div>
    
    <script src="../_static/revealjs4/dist/reveal.js"></script>
    
    
      <script src="../_static/revealjs4/plugin/notes/notes.js"></script>
      <script src="../_static/revealjs4/plugin/highlight/highlight.js"></script>
      <script src="../_static/revealjs4/plugin/math/math.js"></script>
      
    
    <script>
        var revealjsConfig = new Object();
        Object.assign(revealjsConfig, JSON.parse('{"controls": true, "progress": true, "hash": true, "center": true, "transition": "slide", "slideNumber": true}'));
        
        
        
          revealjsConfig.plugins = [
            RevealNotes,RevealHighlight,RevealMath,
          ];
        
        // More info https://github.com/hakimel/reveal.js#configuration
        Reveal.initialize(revealjsConfig);
    </script>

  </body>
</html>