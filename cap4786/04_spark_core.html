
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <title>Introduction to Big Data Analytics</title>
    <link rel="stylesheet" type="text/css" href="../_static/revealjs4/dist/reveal.css" />
    <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/revealjs4/plugin/highlight/zenburn.css" />
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

  </head><body>
    <div class="reveal">
        <div class="slides" role="main">
            <section data-background-color="white" data-background-image="../_static/logo.png" data-background-size="50%" data-background-position="top center" data-background-repeat="no-repeat">
<h1>Introduction to Big Data Analytics</h1>
<div class="subtitle docutils container">
<p><strong>Chapter 3: Spark Core</strong></p>
<p>Xingang (Ian) Fang</p>
</div>
</section>
<section >
<h2>Spark Overview</h2>
<ul class="simple">
<li><p>Spark is an in-memory cluster computing framework for large-scale data
processing.</p></li>
<li><p>Key characteristics</p>
<ul>
<li><p>easy to use</p></li>
<li><p>fast</p></li>
<li><p>general-purpose</p></li>
<li><p>scalable</p></li>
<li><p>fault-tolerant</p></li>
</ul>
</li>
</ul>
</section>
<section >
<h2>Characteristics</h2>
<ul class="simple">
<li><p>Easy to use</p>
<ul>
<li><p>Spark provides high-level APIs in Java, Scala, Python, and R</p></li>
<li><p>Much more concise than Hadoop MapReduce code</p></li>
</ul>
</li>
<li><p>Fast</p>
<ul>
<li><p>Spark can run programs up to 100x faster than Hadoop MapReduce in memory,
or 10x faster on disk.</p></li>
</ul>
</li>
<li><p>General-purpose</p>
<ul>
<li><p>While Hadoop is a platform allowing many third-party applications to run
on it, Spark is a general-purpose computing engine with most essential functionalities built-in.</p></li>
<li><p>Spark is designed to cover a wide range of workloads such as batch
applications, iterative algorithms, interactive queries, and streaming. Spark also supports third-party tools and libraries although its core components are more frequently used.</p></li>
</ul>
</li>
<li><p>Scalable</p>
<ul>
<li><p>Spark can run on clusters with thousands of nodes.</p></li>
<li><p>Can always add more nodes to your cluster to handle more data.</p></li>
</ul>
</li>
<li><p>Fault-tolerant</p>
<ul>
<li><p>Spark provides fault tolerance through lineage information stored in the
memory.</p></li>
<li><p>If a partition of an RDD is lost, Spark will recompute it using the
lineage information.</p></li>
<li><p>Automatically handle failed or slow nodes.</p></li>
</ul>
</li>
</ul>
</section>
<section>
<section >
<h2>Spark Architecture</h2>
<img alt="../_images/spark-arch.png" src="../_images/spark-arch.png" />
</section>
<section >
<h3>Spark Components</h3>
<ul class="simple">
<li><p>Worker: Computer nodes that provide CPU, memory and storage resources to a
spark application.</p></li>
<li><p>Cluster Manager: used to acquire cluster resources for executing a job.
Options: Standalone, Mesos, or YARN</p></li>
<li><p>Driver Program: Provides data processing code that are executed on worker
nodes.</p></li>
<li><p>Executor: A JVM process that spark creates on each worker for an application.</p></li>
<li><p>Task: A smallest unit of work that spark sends to an executor.</p></li>
</ul>
</section>
</section>
<section>
<section >
<h2>Application Execution</h2>
<ul class="simple">
<li><p>Key concepts</p>
<ul>
<li><p>Shuffle:  Redistributes data among cluster of nodes.</p></li>
<li><p>Job: A job is a set of computations that spark performs to return results
to a driver program.</p></li>
<li><p>Stage: A stage is a collection of tasks. Spark splits a task into a DAG
(Directed acyclic graphs) of stages.</p></li>
</ul>
</li>
<li><p>Execution flow</p>
<ol class="arabic simple">
<li><p>The driver program connects to the cluster manager to acquire resources.</p></li>
<li><p>Algorithms are submitted as jobs</p></li>
<li><p>Spark breaks the job into a DAG (directed acyclic graph) of stages</p></li>
<li><p>Stages are scheduled on executors</p></li>
<li><p>Executors run tasks and return results to the driver program</p></li>
</ol>
</li>
</ul>
</section>
<section >
<h3>Execution Flow</h3>
<img alt="Spark Execution" src="../_images/spark-execution.png" />
<div class="footnote docutils container">
<p>Credit: <a class="reference external" href="https://data-flair.training/blogs/how-apache-spark-works/">https://data-flair.training/blogs/how-apache-spark-works/</a></p>
</div>
</section>
</section>
<section >
<h2>Data Source</h2>
<ul class="simple">
<li><p>Spark does not provide its own storage system.</p></li>
<li><p>It can read data from various sources</p>
<ul>
<li><p>Distributed file systems: HDFS, Azure Data Lake</p></li>
<li><p>Object storage: Amazon S3, Azure Blob Storage, Google Cloud Storage</p></li>
<li><p>NoSQL databases: Cassandra, HBase, MongoDB, Couchbase</p></li>
<li><p>Traditional relational databases: MySQL, PostgreSQL, Oracle, SQL Server</p></li>
</ul>
</li>
<li><p>Spark supports structured, semi-structured, and unstructured data.</p></li>
<li><p>Spark also supports various file formats such as Parquet, Avro, ORC, JSON,
and CSV.</p></li>
<li><p>Data ingestion through data APIs, streaming, etc.</p></li>
</ul>
</section>
<section >
<h2>Spark Application Programming Interface (API)</h2>
<ul class="simple">
<li><p>Spark provides APIs in Java, Scala, Python, and R.</p></li>
<li><p>Two fundamental components</p>
<ul>
<li><p>SparkContext: Entry point to any spark functionality.</p></li>
<li><p>RDD: Resilient Distributed Dataset, the fundamental data structure of
Spark.</p></li>
</ul>
</li>
<li><p>Spark SQL provides high-level DataFrame and Dataset APIs built on top of
RDDs.</p></li>
</ul>
</section>
<section >
<h2>SparkContext Object</h2>
<ul>
<li><p>The entry point to any spark functionality.</p></li>
<li><p>Available as the variable <code class="docutils literal notranslate"><span class="pre">sc</span></code> in the spark shell.</p></li>
<li><p>In standalone applications, you create and configure a SparkContext object.</p>
<pre><code data-trim data-noescape class="scala">val conf = new SparkConf().setAppName(&quot;MyApp&quot;).setMaster(&quot;spark://host:port&quot;)
val sc = new SparkContext(conf)</code></pre>
</li>
</ul>
</section>
<section>
<section >
<h2>Resilient Distributed Dataset (RDD)</h2>
<ul class="simple">
<li><p>RDD is the fundamental data structure of Spark to model collections of
objects distributed across a cluster.</p></li>
<li><p>Characteristics</p>
<ul>
<li><p>Immutable: Once created, an RDD cannot be changed.</p></li>
<li><p>Distributed: Data is distributed across multiple nodes in a cluster as
partitions.</p></li>
<li><p>Fault-tolerant: If a partition of an RDD is lost, Spark will recompute it
using lineage information.</p></li>
<li><p>In-memory computation: Spark stores intermediate results in memory.</p></li>
<li><p>Strongly typed: RDDs are strongly typed with Scala.</p></li>
<li><p>Interface: available as Scala abstract class with concrete implementations
such as ParallelCollectionRDD, HadoopRDD, etc.</p></li>
<li><p>Lazy evaluation: Spark does not compute the result until an action is
called.</p></li>
</ul>
</li>
<li><p>RDD resembles local collection types in terms of operations</p></li>
</ul>
</section>
<section >
<h3>RDD Creation</h3>
<ul>
<li><p>Two ways to create RDDs</p>
<ul>
<li><p>Parallelizing an existing collection in your driver program.</p>
<ul class="simple">
<li><p>Using the <code class="docutils literal notranslate"><span class="pre">parallelize</span></code> method.</p></li>
<li><p>Only for small datasets that can fit in memory of a single node.</p></li>
</ul>
<pre><code data-trim data-noescape class="scala">val data = (1 to 10000).toList
val distData = sc.parallelize(data)</code></pre>
</li>
<li><p>Load data from an external storage system such as HDFS, S3, etc.</p>
<ul class="simple">
<li><p>SparkContext provides methods to read data from various sources.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">textFile</span></code>: Read text file(s) from HDFS, S3, etc. with each line as
an element.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">wholeTextFiles</span></code>: Read text file(s) from HDFS, S3, etc. with each
file as an element.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sequenceFile</span></code>: Read key-value pairs from Hadoop sequence files.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section >
<h3>RDD Operations</h3>
<ul class="simple">
<li><p>Transformations and actions are extended concepts corresponding to map and
reduce in Hadoop MapReduce.</p></li>
<li><p>Transformations</p>
<ul>
<li><p>Create a new RDD from an existing RDD.</p></li>
<li><p>map is a special case that applies a function to each element of an RDD to
provide another RDD with the same size.</p></li>
</ul>
</li>
<li><p>Actions</p>
<ul>
<li><p>Return an object to the driver program after running a computation on the
RDD.</p></li>
</ul>
</li>
</ul>
</section>
<section >
<h3>RDD Operations Cont.d 1</h3>
<ul class="simple">
<li><p>Common RDD types that require different operations</p>
<ul>
<li><p>List like: RDD of a list of elements</p></li>
<li><p>Map like: RDD of key-value pairs</p></li>
</ul>
</li>
<li><p>Common transformations</p>
<ul>
<li><p>For normal list like RDD: map, filter, flatMap, distinct, union,
intersection, subtract, cartesian, etc.</p></li>
<li><p>For map like RDD: mapValues, flatMapValues, keys, values, reduceByKey,
groupByKey, sortByKey, etc.</p></li>
</ul>
</li>
<li><p>Common actions</p>
<ul>
<li><p>For normal list like RDD: reduce, collect, count, first, take, takeSample,
takeOrdered, saveAsTextFile, etc.</p></li>
<li><p>For map like RDD: countByKey, collectAsMap, lookup, etc.</p></li>
</ul>
</li>
</ul>
</section>
<section >
<h3>Lazy Evaluation</h3>
<ul class="simple">
<li><p>A.k.a lazy operation or call by need.</p></li>
<li><p>Transformations and RDD creation</p>
<ul>
<li><p>Spark does not compute the result immediately.</p></li>
<li><p>Only lineage information is recorded.</p></li>
<li><p>Each worker will apply the transformation to its partition of the RDD.</p></li>
<li><p>Run in parallel on different workers.</p></li>
</ul>
</li>
<li><p>Actions</p>
<ul>
<li><p>Action will trigger the execution of the unfinished upstream
transformations.</p></li>
<li><p>Requires workers to interchange data among themselves.</p></li>
<li><p>Cannot be easily parallelized.</p></li>
<li><p>Spark will optimize the execution plan before running the job.</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section >
<h2>Caching</h2>
<p>As Spark RDDs are computed on-demand, it is possible that the same RDD is
computed multiple times. To avoid this, Spark provides caching. When an RDD is
cached, it is stored in memory across the nodes in the cluster. This allows
subsequent actions on the RDD to be computed faster.</p>
<ul class="simple">
<li><p>Two ways to cache an RDD</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">cache</span></code> method: Cache the RDD in memory.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">persist</span></code> method: Cache the RDD in memory or on disk</p></li>
</ul>
</li>
<li><p>Fault tolerance: If an RDD is lost, Spark will recompute it using lineage
information.</p></li>
<li><p>Memory management: Spark will automatically evict old data to make space for
new data.</p></li>
</ul>
</section>
<section >
<h2>Spark Jobs</h2>
<ul class="simple">
<li><p>A job is a set of computations that Spark performs to return the results of an action to a driver program.</p></li>
<li><p>An application can have multiple jobs.</p></li>
<li><p>Spark breaks a job into a DAG (directed acyclic graph) of stages.</p></li>
<li><p>Each stage is a collection of tasks.</p></li>
<li><p>Tasks are grouped into stages based on the shuffle boundary.</p></li>
<li><p>Stages are scheduled on executors to be executed in parallel.</p></li>
</ul>
</section>
<section >
<h2>Shared Variable</h2>
<ul class="simple">
<li><p>A share-nothing model by default</p></li>
<li><p>Two types of shared variables</p>
<ul>
<li><p>Broadcast variables: Efficiently distribute large read-only values to
workers.</p></li>
<li><p>Accumulators: Aggregate values from workers back to the driver program.</p></li>
</ul>
</li>
</ul>
</section>

        </div>
    </div>
    
    <script src="../_static/revealjs4/dist/reveal.js"></script>
    
    
      <script src="../_static/revealjs4/plugin/notes/notes.js"></script>
      <script src="../_static/revealjs4/plugin/highlight/highlight.js"></script>
      <script src="../_static/revealjs4/plugin/math/math.js"></script>
      
    
    <script>
        var revealjsConfig = new Object();
        Object.assign(revealjsConfig, JSON.parse('{"controls": true, "progress": true, "hash": true, "center": true, "transition": "slide", "slideNumber": true}'));
        
        
        
          revealjsConfig.plugins = [
            RevealNotes,RevealHighlight,RevealMath,
          ];
        
        // More info https://github.com/hakimel/reveal.js#configuration
        Reveal.initialize(revealjsConfig);
    </script>

  </body>
</html>